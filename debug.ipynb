{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import dateutil\n",
    "import json\n",
    "import pathlib\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import Any, Callable, Dict, Iterable, List, Tuple, Optional, Set, Union\n",
    "\n",
    "import elasticsearch\n",
    "import pandas as pd\n",
    "import requests\n",
    "import requests_cache\n",
    "import spacy\n",
    "import yfinance as yf\n",
    "from dotenv import load_dotenv\n",
    "from elasticsearch_dsl import connections, Document, Date, Keyword, Q, Search, Text, Range, Integer\n",
    "from flashtext import KeywordProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/entity_knowledge_in_bert/bert_entity\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/entity_knowledge_in_bert/bert_entity\n",
    "from vocab import Vocab\n",
    "from data_loader_wiki import EDLDataset\n",
    "\n",
    "class args:\n",
    "    data_version_name = \"dummy\"\n",
    "    train_loc_file = \"train.loc\"\n",
    "    device = \"cpu\"\n",
    "    label_size = 1024\n",
    "    uncased = True\n",
    "    train_data_dir = \"data\"\n",
    "    wiki_lang_version = \"enwiki\"\n",
    "    batch_size = 1\n",
    "    data_workers = 4\n",
    "    collect_most_popular_labels_steps = 1\n",
    "    top_rnns = False\n",
    "    project = False\n",
    "    entity_embedding_size = 768\n",
    "    sparse = True\n",
    "    out_device = 'cpu'\n",
    "    finetuning = 3\n",
    "\n",
    "vocab = Vocab(args)\n",
    "train_dataset = EDLDataset(\n",
    "    args, split=\"train\", vocab=vocab, device=args.device, label_size=args.label_size)\n",
    "iterator = train_dataset.get_data_iter(args, batch_size=args.batch_size, vocab=vocab, train=True)\n",
    "\n",
    "for x in iterator:\n",
    "#     batch_token_ids, label_ids, label_probs, eval_mask, _, _, orig_batch, loaded_batch = x\n",
    "#     print(x)\n",
    "#     print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_token_ids, label_ids, label_probs, eval_mask, _, _, orig_batch, loaded_batch = x\n",
    "\n",
    "# batch_token_ids, label_ids\n",
    "# orig_batch\n",
    "# eval_mask\n",
    "# loaded_batch\n",
    "# label_probs.shape\n",
    "# label_ids.shape\n",
    "# batch_token_ids.shape\n",
    "# label_ids\n",
    "# for p in label_probs[0]:\n",
    "#     print(p)\n",
    "# label_probs[0, 13]\n",
    "# for p in label_probs[0]:\n",
    "#     print(p.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Net\n",
    "model = Net(args, vocab.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1312, 51457,   184,  ..., 32567, 10045, 44862])\n"
     ]
    }
   ],
   "source": [
    "# print(model)\n",
    "y = model(batch_token_ids, label_ids, label_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, _, y_hat, _, _, enc = y\n",
    "# logits.shape\n",
    "# logits\n",
    "# y_hat\n",
    "\n",
    "\n",
    "# vocab.size()\n",
    "label_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.eval()\n",
    "encoded_layers, _ = bert(batch_token_ids)\n",
    "# batch_token_ids.shape\n",
    "enc = encoded_layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(51459, 768, sparse=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "out = nn.Embedding(num_embeddings=vocab.size(), embedding_dim=768, sparse=True)\n",
    "out.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(41.4637, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _.shape\n",
    "# encoded_layers[-1].shape\n",
    "embeded_labels = out(label_ids) # (1024, 768)\n",
    "embeded_labels = embeded_labels.transpose(0, 1)  # (768, 1024)\n",
    "logits = enc.matmul(embeded_labels) # (1, 254, 1024)\n",
    "# logits.argmax(-1)  # (1,254)\n",
    "logits[0,0,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3413, 0.5543, 0.3464, 0.6351, 0.4112]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "x = torch.randn(1, 5)  # (1, 5)\n",
    "y = torch.randint(5, (1,), dtype=torch.int64)  # (1, )\n",
    "loss = F.cross_entropy(x, y)\n",
    "# loss\n",
    "# x.shape, y.shape\n",
    "# x, y\n",
    "\n",
    "x = torch.randn((1, 5))\n",
    "y = torch.rand((1, 5))\n",
    "# F.binary_cross_entropy(torch.sigmoid(x), y)\n",
    "# x, y\n",
    "# F.binary_cross_entropy(x, y)\n",
    "# torch.sigmoid(x)\n",
    "y\n",
    "y.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_corpus_mentions_cooccurs(mentions: Dict[str, List[str]]) -> Any:\n",
    "    mention2idx = get_mention2idx_dict(mentions)\n",
    "\n",
    "    def count_cooccur(bag: Iterable[int], cooccur: Dict[int, Dict[int, int]] = defaultdict(lambda: defaultdict(int))) -> Dict[int, Dict[int, int]]:\n",
    "        for a in bag:\n",
    "            for b in bag:\n",
    "                cooccur[a][b] += 1\n",
    "        return cooccur\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    processor = KeywordProcessor()\n",
    "    processor.add_keywords_from_dict(mentions)\n",
    "\n",
    "    sent_cooccur, atk_cooccur = defaultdict(\n",
    "        lambda: defaultdict(int)), defaultdict(lambda: defaultdict(int))\n",
    "    atk_bags = []\n",
    "    for i, hit in enumerate(es.scan_scraper_page(\"*cnbc*\")):\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        if i > 1000:\n",
    "            break\n",
    "        sents = []\n",
    "        try:\n",
    "            sents.append(hit.article_title)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sents += [sent.text for sent in nlp(hit.article_text).sents]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        atk_bag = set()\n",
    "        for sent in sents:\n",
    "            kws = processor.extract_keywords(sent, span_info=False)\n",
    "            sent_bag = [mention2idx[kw] for kw in kws]\n",
    "            sent_cooccur = count_cooccur(sent_bag, sent_cooccur)\n",
    "            atk_bag |= set(sent_bag)\n",
    "        atk_cooccur = count_cooccur(atk_bag, atk_cooccur)\n",
    "        atk_bags.append(atk_bag)\n",
    "\n",
    "    return (\n",
    "        # 在一個句子中，兩個entity同時出現的次數: {a: {a: 4, b: 2, c:1}, b: {...}}\n",
    "        dict(sent_cooccur),\n",
    "        # 在一篇文章中，兩個entity同時出現的次數: {a: {a: 4, b: 2, c:1}, b: {...}}\n",
    "        dict(atk_cooccur),\n",
    "        # 在一篇文章中出現的entities集合成一個bag: [{a, b, c}, {a, c, f}, ...]\n",
    "        set(map(frozenset, atk_bags)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/workspace/entity_knowledge_in_bert/app/outputs/wiki_mentions.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ab34442321ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/workspace/entity_knowledge_in_bert/app/outputs/wiki_mentions.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/workspace/entity_knowledge_in_bert/app/outputs/wiki_mentions.json'"
     ]
    }
   ],
   "source": [
    "filepath = \"/workspace/entity_knowledge_in_bert/app/outputs/wiki_mentions.json\"\n",
    "\n",
    "with open(filepath, encoding='utf-8') as f:\n",
    "    mentions = json.load(f)\n",
    "\n",
    "mentions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
