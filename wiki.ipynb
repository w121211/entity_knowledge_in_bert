{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install sparqlwrapper\n",
    "# !pip install wikidataintegrator\n",
    "# !pip install --upgrade --force-reinstall git+https://github.com/vi3k6i5/flashtext.git\n",
    "# !pip install rdflib\n",
    "# !pip install requests_cache\n",
    "# !conda install -y -c ranaroussi yfinance\n",
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wiki-data\n",
    "\n",
    "wiki-data api  \n",
    "https://query.wikidata.org/  \n",
    "https://sqid.toolforge.org/#/view?id=P452  \n",
    "\n",
    "https://www.wikidata.org/w/api.php?action=wbgetclaims&entity=Q66&format=jsonfm   \n",
    "https://www.wikidata.org/w/api.php?action=wbgetentities&ids=Q66&format=jsonfm \n",
    "\n",
    "```sparql\n",
    "# 列items有ticker property\n",
    "SELECT ?item ?itemLabel ?value ?valueLabel\n",
    "WHERE \n",
    "{\n",
    "  # wdt:P249 (ticker symbol)\n",
    "  ?item wdt:P249 ?value\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "LIMIT 10\n",
    "\n",
    "#\n",
    "?horse wdt:P31/wdt:P279* wd:Q726 .\n",
    " wd:Q180816\t\n",
    "```\n",
    "\n",
    "# Wiki\n",
    "\n",
    "wiki-page api  \n",
    "https://en.wikipedia.org/wiki/Special:ApiSandbox#action=query&prop=revisions&titles=Pet_door&rvslots=*&rvprop=content&formatversion=2\n",
    "\n",
    "Query wiki-page  \n",
    "https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro&explaintext&redirects=1&pageids=21721040\n",
    "\n",
    "用wikidata-id找對應的wikipage  \n",
    "https://www.wikidata.org/w/api.php?action=wbgetentities&format=xml&props=sitelinks&ids=Q19675&sitefilter=frwiki\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/entity_knowledge_in_bert\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Self-driving_car'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 從具有company屬性的節點開始，抓5層mentions\n",
    "\n",
    "# 取得上市公司entities\n",
    "#  方法1：掃過整個wikipedia-dump，找出具有company屬性的entities\n",
    "#  方法2（採用）：query wikidata找company屬性的entities\n",
    "\n",
    "# 取得跟公司關聯的mentions（n階層）\n",
    "# 掃wikipedia-dump，從company的wiki-page開始抓裡面的mentions\n",
    "#  將新加入的mentions設為next_entities，重複抓取n次（=爬n層）\n",
    "\n",
    "%cd /workspace/entity_knowledge_in_bert\n",
    "import requests\n",
    "import requests_cache\n",
    "requests_cache.install_cache('cache')\n",
    "\n",
    "j = requests.get(\"https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&props=sitelinks&ids=Q741490\").json()\n",
    "a = j['entities']['Q741490']['sitelinks']['enwiki']['title'].replace(' ', '_')\n",
    "a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認爬出來的entities\n",
    "#  先確認一下entities的內容，是否足夠滿足#tag需求（ie足夠豐富、多元）\n",
    "#  假如不夠，可考慮採關鍵字方式與wiki-entity-dictionary做對照\n",
    "\n",
    "# 用flashtext從news-corpus中抓bags\n",
    "#  sentence-bag: 同時出現在1句子中的entities\n",
    "#  paragraph-bag: 同時出現在1段落中的entities\n",
    "#  sentence-coocuur-dict\n",
    "#  paragraph-coocuur-dict\n",
    "#  freq-daily-count-dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"#aaa\"\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbpedia infobox\n",
    "!cd /workspace/entity_knowledge_in_bert\n",
    "import bz2\n",
    "# import rdflib\n",
    "# from rdflib import Graph\n",
    "\n",
    "# f = \"./downloads/infobox-property-definitions_lang=en.ttl.bz2\"\n",
    "# f = \"./downloads/mappingbased-objects_lang=en.ttl.bz2\"\n",
    "# f = \"./downloads/wikipedia-links_lang=en.ttl.bz2\"\n",
    "f = \"./downloads/redirects_lang=en.ttl.bz2\"\n",
    "# f = \"./downloads/enwiki-latest-pages-articles1.xml-p1p30303.bz2\"\n",
    "# f = \"./downloads/disambiguations_en.ttl.bz2\"\n",
    "\n",
    "# g = Graph()\n",
    "# g.parse(f, format=\"turtle\"); #what is right value for the format\n",
    "\n",
    "with bz2.BZ2File(f, \"rb\") as f:\n",
    "    for i, l in enumerate(f):\n",
    "#         if i > 10:\n",
    "#             break\n",
    "        line = l.decode().strip()\n",
    "#         print(line)\n",
    "        if line[0] == '#':\n",
    "            continue\n",
    "        try:\n",
    "            s, p, o, _ = line.split(\" \")\n",
    "#             print(s, p, o)\n",
    "        except ValueError:\n",
    "            print(line)\n",
    "        \n",
    "#         if \"http://dbpedia.org/resource/Google\" in s or \"http://dbpedia.org/resource/Google\" in o:\n",
    "        if \"<http://dbpedia.org/resource/boeing>\" in s:\n",
    "            print(s, p, o)\n",
    "\n",
    "#         if \"http://dbpedia.org/resource/Google\" in line_decoded:\n",
    "#             print(line_decoded)\n",
    "        \n",
    "# #             for line in tqdm.tqdm(file):\n",
    "# #                 line_decoded = line.decode().strip()\n",
    "# #                 matcher_match = matcher.match(line_decoded)\n",
    "# #                 if matcher_match:\n",
    "# #                     if matcher_match.group(match_val) in redirects:\n",
    "# #                         a_to_b[\n",
    "# #                             postproc_key(matcher_match.group(match_key))\n",
    "# #                         ].append(redirects[postproc_val(matcher_match.group(match_val))])\n",
    "# #                     else:\n",
    "# #                         a_to_b[\n",
    "# #                             postproc_key(matcher_match.group(match_key))\n",
    "# #                         ].append(postproc_val(matcher_match.group(match_val)))\n",
    "# #         return a_to_b\n",
    "# for s, p, o in g:\n",
    "#     print(s, p, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'car']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flashtext\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "text = \"Car When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously\"\n",
    "\n",
    "keyword_processor = KeywordProcessor()\n",
    "# keyword_processor.add_keyword('Big Apple', 'New York')\n",
    "# keyword_processor.add_keyword('apple', 'apple watch')\n",
    "# keyword_processor.add_keyword(\"Google\")\n",
    "# keyword_processor.add_keyword(\"Sebastian Thrun\")\n",
    "keyword_processor.add_keyword(\"Self driving car\")\n",
    "keyword_processor.add_keyword(\"cars\", \"car\")\n",
    "keyword_processor.add_keyword(\"car\", \"car\")\n",
    "\n",
    "# keyword_processor.add_keyword(\"apple \", \"apple-watch\")\n",
    "\n",
    "# keywords_found = keyword_processor.extract_keywords(text, span_info=True, max_cost=1)\n",
    "keywords_found = keyword_processor.extract_keywords(text, span_info=False, max_cost=1)\n",
    "keywords_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "def get_results(endpoint_url, query):\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    # TODO adjust user agent; see https://w.wiki/CX6\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qeury具有stock exchange（wdt:P414）屬性的items\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT ?item ?itemLabel ?value ?valueLabel\n",
    "WHERE \n",
    "{\n",
    "  ?item wdt:P414 ?value\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "results = get_results(endpoint_url, query)\n",
    "# results[\"results\"][\"bindings\"]\n",
    "\n",
    "# 將results儲存在本機\n",
    "import json\n",
    "with open('data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/flair\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q66'},\n",
       " 'value': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q13677'},\n",
       " 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Boeing'},\n",
       " 'valueLabel': {'xml:lang': 'en',\n",
       "  'type': 'literal',\n",
       "  'value': 'New York Stock Exchange'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /workspace/flair\n",
    "import json\n",
    "\n",
    "with open('data.json',  encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# len(results[\"results\"][\"bindings\"])\n",
    "data[\"results\"][\"bindings\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得entity statements（claims）\n",
    "import requests\n",
    "import \n",
    "\n",
    "entity = 'Q66'\n",
    "endpoint = f'https://www.wikidata.org/w/api.php?action=wbgetclaims&entity={entity}&format=json'\n",
    "\n",
    "r = requests.get(endpoint)\n",
    "d = r.json()\n",
    "# d['claims']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得entity statements（claims） - 用wikidataintegrator\n",
    "from wikidataintegrator import wdi_core\n",
    "\n",
    "# wdid='Q707530'\n",
    "wdid=\"Q741490\"\n",
    "my_first_wikidata_item = wdi_core.WDItemEngine(wd_item_id=wdid, search_only=True)\n",
    "\n",
    "# to check successful installation and retrieval of the data, you can print the json representation of the item\n",
    "d = my_first_wikidata_item.get_wd_json_representation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d['claims']['P169'][-1]\n",
    "# d['claims']\n",
    "# ['mainsnak']['datavalue']['value']['id']\n",
    "# my_first_wikidata_item.mrh\n",
    "# d['claims']\n",
    "# del d['claims']\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /workspace/entity_knowledge_in_bert/wikiextractor-wikimentions\n",
    "# !python WikiExtractor.py --json --filter_disambig_pages --processes 2 --collect_links ../downloads/enwiki-latest-pages-articles1.xml-p1p30303.bz2 -o test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/entity_knowledge_in_bert\n",
      "[[26, 46, \"political philosophy\", \"political philosophy\"], [51, 59, \"movement\", \"Political movement\"], [108, 117, \"hierarchy\", \"hierarchy\"], [153, 158, \"state\", \"State (polity)\"], [224, 244, \"history of anarchism\", \"history of anarchism\"], [258, 268, \"prehistory\", \"prehistory\"], [366, 371, \"realm\", \"realm\"], [376, 382, \"empire\", \"empire\"], [433, 443, \"skepticism\", \"skepticism\"], [752, 764, \"emancipation\", \"emancipation\"], [774, 802, \"anarchist schools of thought\", \"anarchist schools of thought\"], [901, 918, \"Spanish Civil War\", \"Spanish Civil War\"], [948, 973, \"anarchism's classical era\", \"Classical era of anarchism\"], [1530, 1536, \"praxis\", \"Praxis (process)\"], [1910, 1914, \"-ism\", \"-ism\"], [1961, 1968, \"anarchy\", \"anarchy\"], [2080, 2097, \"French Revolution\", \"French Revolution\"], [2256, 2270, \"William Godwin\", \"William Godwin\"], [2287, 2303, \"Wilhelm Weitling\", \"Wilhelm Weitling\"], [2546, 2568, \"Pierre-Joseph Proudhon\", \"Pierre-Joseph Proudhon\"], [2687, 2701, \"libertarianism\", \"libertarianism\"], [2875, 2913, \"individualistic free-market philosophy\", \"Individualist anarchism\"], [2933, 2954, \"free-market anarchism\", \"free-market anarchism\"], [2990, 3013, \"opposition to the state\", \"opposition to the state\"], [3318, 3327, \"authority\", \"authority\"], [3332, 3357, \"hierarchical organization\", \"hierarchical organization\"], [3373, 3378, \"state\", \"State (polity)\"], [3380, 3390, \"capitalism\", \"Anarchism and capitalism\"], [3392, 3403, \"nationalism\", \"Anarchism and nationalism\"], [3423, 3434, \"institution\", \"institution\"], [3459, 3474, \"human relations\", \"human relations\"], [3507, 3528, \"voluntary association\", \"voluntary association\"], [3533, 3540, \"freedom\", \"freedom\"], [3548, 3564, \"decentralisation\", \"decentralisation\"], [3795, 3807, \"a posteriori\", \"A priori and a posteriori\"], [4093, 4098, \"China\", \"History of China\"], [4103, 4109, \"Greece\", \"Ancient Greece\"], [4121, 4144, \"philosophical anarchism\", \"philosophical anarchism\"], [4216, 4235, \"Taoist philosophers\", \"Taoism\"], [4236, 4247, \"Zhuang Zhou\", \"Zhuang Zhou\"], [4252, 4257, \"Laozi\", \"Laozi\"], [4348, 4357, \"Aeschylus\", \"Aeschylus\"], [4362, 4371, \"Sophocles\", \"Sophocles\"], [4389, 4397, \"Antigone\", \"Antigone\"], [4460, 4477, \"personal autonomy\", \"autonomy\"], [4479, 4487, \"Socrates\", \"Socrates\"], [4597, 4603, \"Cynics\", \"Cynicism (philosophy)\"], [4626, 4631, \"nomos\", \"Nomos (sociology)\"], [4704, 4710, \"physis\", \"physis\"], [4714, 4720, \"Stoics\", \"Stoics\"], [4857, 4868, \"Middle Ages\", \"Middle Ages\"], [4951, 4963, \"Muslim world\", \"Muslim world\"], [5031, 5050, \"religious anarchism\", \"Anarchism and religion#Religious anarchism and anarchist themes in religions\"], [5059, 5074, \"Sasanian Empire\", \"Sasanian Empire\"], [5076, 5082, \"Mazdak\", \"Mazdak\"], [5097, 5116, \"egalitarian society\", \"egalitarianism\"], [5125, 5146, \"abolition of monarchy\", \"abolition of monarchy\"], [5184, 5191, \"Kavad I\", \"Kavad I\"], [5197, 5202, \"Basra\", \"Basra\"], [5367, 5378, \"Renaissance\", \"Renaissance\"], [5398, 5407, \"reasoning\", \"Rationalism\"], [5412, 5420, \"humanism\", \"humanism\"], [5530, 5543, \"Enlightenment\", \"Age of Enlightenment\"], [5628, 5645, \"French Revolution\", \"French Revolution\"], [5675, 5682, \"Enrag\\u00e9s\", \"Enrag\\u00e9s\"], [5839, 5853, \"William Godwin\", \"William Godwin\"], [5863, 5886, \"philosophical anarchism\", \"philosophical anarchism\"], [5933, 5944, \"Max Stirner\", \"Max Stirner\"], [5992, 6014, \"Pierre-Joseph Proudhon\", \"Pierre-Joseph Proudhon\"], [6027, 6036, \"mutualism\", \"Mutualism (economic theory)\"], [6127, 6152, \"Spanish Civil War of 1936\", \"Spanish Civil War\"], [6177, 6192, \"Mikhail Bakunin\", \"Mikhail Bakunin\"], [6201, 6223, \"collectivist anarchism\", \"collectivist anarchism\"], [6240, 6278, \"International Workingmen's Association\", \"International Workingmen's Association\"], [6462, 6471, \"Karl Marx\", \"Karl Marx\"], [6555, 6570, \"Jura Federation\", \"Jura Federation\"], [6602, 6612, \"mutualists\", \"Mutualism (economic theory)\"], [6630, 6645, \"state socialism\", \"state socialism\"], [6668, 6681, \"abstentionism\", \"abstentionism\"], [6810, 6829, \"1872 Hague Congress\", \"1872 Hague Congress\"], [7002, 7025, \"St. Imier International\", \"Anarchist St. Imier International\"], [7050, 7065, \"Peter Kropotkin\", \"Peter Kropotkin\"], [7104, 7121, \"anarcho-communism\", \"anarcho-communism\"], [7207, 7220, \"Paris Commune\", \"Paris Commune\"], [7604, 7617, \"Latin America\", \"Latin America\"], [7619, 7628, \"Argentina\", \"Anarchism in Argentina\"], [7650, 7669, \"anarcho-syndicalism\", \"anarcho-syndicalism\"], [7802, 7820, \"political violence\", \"political violence\"], [7852, 7874, \"propaganda of the deed\", \"propaganda of the deed\"], [7981, 7991, \"Communards\", \"Communards\"], [8219, 8229, \"Illegalism\", \"Illegalism\"], [8278, 8296, \"Russian Revolution\", \"Russian Revolution\"], [8335, 8341, \"Whites\", \"White movement\"], [8389, 8409, \"Bolshevik government\", \"Bolshevik government\"], [8511, 8530, \"Kronstadt rebellion\", \"Kronstadt rebellion\"], [8535, 8548, \"Nestor Makhno\", \"Nestor Makhno\"], [8567, 8581, \"Free Territory\", \"Free Territory\"], [8674, 8685, \"platformism\", \"platformism\"], [8690, 8709, \"synthesis anarchism\", \"synthesis anarchism\"], [8911, 8929, \"October Revolution\", \"October Revolution\"], [8948, 8965, \"Russian Civil War\", \"Russian Civil War\"], [9004, 9021, \"communist parties\", \"Communist party\"], [9170, 9201, \"General Confederation of Labour\", \"General Confederation of Labour (France)\"], [9206, 9237, \"Industrial Workers of the World\", \"Industrial Workers of the World\"], [9279, 9302, \"Communist International\", \"Communist International\"], [9312, 9329, \"Spanish Civil War\", \"Spanish Civil War\"], [9360, 9363, \"CNT\", \"Confederaci\\u00f3n Nacional del Trabajo\"], [9368, 9371, \"FAI\", \"Federaci\\u00f3n Anarquista Ib\\u00e9rica\"], [9457, 9474, \"Spanish anarchism\", \"Spanish anarchism\"], [9664, 9673, \"Barcelona\", \"Barcelona\"], [9720, 9733, \"collectivised\", \"Collective farming\"], [9748, 9760, \"Soviet Union\", \"Soviet Union\"], [9917, 9925, \"May Days\", \"May Days\"], [9929, 9942, \"Joseph Stalin\", \"Joseph Stalin\"], [9973, 9984, \"Republicans\", \"Republican faction (Spanish Civil War)\"], [9999, 10011, \"World War II\", \"World War II\"], [10151, 10167, \"Marxism\\u2013Leninism\", \"Marxism\\u2013Leninism\"], [10194, 10202, \"Cold War\", \"Cold War\"], [10321, 10333, \"anti-nuclear\", \"Anti-nuclear movement\"], [10335, 10348, \"environmental\", \"Environmental movement\"], [10353, 10371, \"pacifist movements\", \"Peace movement\"], [10377, 10385, \"New Left\", \"New Left\"], [10395, 10422, \"counterculture of the 1960s\", \"counterculture of the 1960s\"], [10457, 10472, \"punk subculture\", \"punk subculture\"], [10506, 10511, \"Crass\", \"Crass\"], [10520, 10531, \"Sex Pistols\", \"Sex Pistols\"], [10553, 10561, \"feminist\", \"feminist\"], [10576, 10592, \"anarcha-feminism\", \"anarcha-feminism\"], [10625, 10648, \"second wave of feminism\", \"second wave of feminism\"], [10738, 10746, \"anti-war\", \"anti-war\"], [10748, 10763, \"anti-capitalist\", \"anti-capitalist\"], [10769, 10796, \"anti-globalisation movement\", \"anti-globalisation movement\"], [10869, 10893, \"World Trade Organization\", \"World Trade Organization\"], [10899, 10913, \"Group of Eight\", \"Group of Eight\"], [10922, 10942, \"World Economic Forum\", \"World Economic Forum\"], [10966, 10972, \"ad hoc\", \"ad hoc\"], [11011, 11021, \"black bloc\", \"black bloc\"], [11034, 11038, \"riot\", \"riot\"], [11043, 11063, \"property destruction\", \"property destruction\"], [11101, 11107, \"police\", \"police\"], [11169, 11185, \"security culture\", \"security culture\"], [11187, 11201, \"affinity group\", \"affinity group\"], [11333, 11366, \"WTO conference in Seattle in 1999\", \"1999 Seattle WTO protests\"], [11432, 11442, \"Zapatistas\", \"Zapatista Army of National Liberation\"], [11525, 11531, \"Rojava\", \"Rojava\"], [11536, 11544, \"de facto\", \"de facto\"], [11546, 11563, \"autonomous region\", \"Permanent autonomous zone\"], [11576, 11581, \"Syria\", \"Syria\"], [11584, 11612, \"Anarchist schools of thought\", \"Anarchist schools of thought\"], [11678, 11694, \"social anarchism\", \"social anarchism\"], [11699, 11722, \"individualist anarchism\", \"individualist anarchism\"], [11817, 11833, \"negative liberty\", \"negative liberty\"], [11919, 11935, \"positive liberty\", \"positive liberty\"], [12008, 12024, \"social ownership\", \"social ownership\"], [12172, 12188, \"anarcha-feminism\", \"anarcha-feminism\"], [12190, 12205, \"green anarchism\", \"green anarchism\"], [12210, 12224, \"post-anarchism\", \"post-anarchism\"], [12343, 12366, \"philosophical anarchism\", \"philosophical anarchism\"], [12401, 12417, \"moral legitimacy\", \"Morality\"], [12602, 12615, \"minimal state\", \"minimal state\"], [12650, 12666, \"moral obligation\", \"moral obligation\"], [12792, 12798, \"ethics\", \"ethics\"], [12866, 12878, \"sectarianism\", \"sectarianism\"], [12911, 12939, \"anarchism without adjectives\", \"anarchism without adjectives\"], [12952, 12962, \"toleration\", \"toleration\"], [13007, 13034, \"Fernando Tarrida del M\\u00e1rmol\", \"Fernando Tarrida del M\\u00e1rmol\"], [13278, 13286, \"far-left\", \"far-left\"], [13294, 13312, \"political spectrum\", \"political spectrum\"], [13326, 13335, \"economics\", \"Anarchist economics\"], [13340, 13356, \"legal philosophy\", \"Anarchist law\"], [13365, 13383, \"anti-authoritarian\", \"anti-authoritarian\"], [13385, 13397, \"anti-statist\", \"anti-statist\"], [13403, 13414, \"libertarian\", \"libertarian\"], [13438, 13445, \"radical\", \"Political radicalism\"], [13446, 13455, \"left-wing\", \"left-wing\"], [13460, 13469, \"socialist\", \"socialist\"], [13482, 13494, \"collectivism\", \"Collectivist anarchism\"], [13496, 13505, \"communism\", \"Anarcho-communism\"], [13507, 13520, \"individualism\", \"Individualist anarchism\"], [13522, 13531, \"mutualism\", \"Mutualism (economic theory)\"], [13537, 13548, \"syndicalism\", \"Anarcho-syndicalism\"], [13562, 13583, \"libertarian socialist\", \"libertarian socialist\"], [13697, 13727, \"anarchist types and traditions\", \"History of anarchism\"], [13787, 13796, \"mutualism\", \"Mutualism (economic theory)\"], [13801, 13814, \"individualism\", \"Individualist anarchism\"], [13878, 13890, \"collectivist\", \"Collectivist anarchism\"], [13892, 13901, \"communist\", \"Anarcho-communism\"], [13907, 13918, \"syndicalist\", \"Anarcho-syndicalism\"], [13997, 14006, \"Mutualism\", \"Mutualism (economic theory)\"], [14086, 14108, \"Pierre-Joseph Proudhon\", \"Pierre-Joseph Proudhon\"], [14127, 14138, \"reciprocity\", \"Reciprocity (cultural anthropology)\"], [14140, 14156, \"free association\", \"Free association (Marxism and anarchism)\"], [14168, 14176, \"contract\", \"contract\"], [14178, 14188, \"federation\", \"federation\"], [14194, 14220, \"credit and currency reform\", \"Monetary reform\"], [14514, 14536, \"Collectivist anarchism\", \"Collectivist anarchism\"], [14605, 14628, \"revolutionary socialist\", \"revolutionary socialist\"], [14672, 14687, \"Mikhail Bakunin\", \"Mikhail Bakunin\"], [14722, 14742, \"collective ownership\", \"collective ownership\"], [14985, 14992, \"Marxism\", \"Marxism\"], [15011, 15042, \"dictatorship of the proletariat\", \"dictatorship of the proletariat\"], [15093, 15110, \"stateless society\", \"stateless society\"], [15112, 15129, \"Anarcho-communism\", \"Anarcho-communism\"], [15256, 15273, \"communist society\", \"communist society\"], [15279, 15295, \"common ownership\", \"common ownership\"], [15324, 15340, \"direct democracy\", \"direct democracy\"], [15348, 15366, \"horizontal network\", \"Horizontalidad\"], [15370, 15391, \"voluntary association\", \"voluntary association\"], [15397, 15413, \"workers' council\", \"workers' council\"], [15480, 15545, \"From each according to his ability, to each according to his need\", \"From each according to his ability, to each according to his need\"], [15618, 15635, \"French Revolution\", \"French Revolution\"], [15703, 15722, \"First International\", \"First International\"], [15778, 15793, \"Peter Kropotkin\", \"Peter Kropotkin\"], [15796, 15815, \"Anarcho-syndicalism\", \"Anarcho-syndicalism\"], [15900, 15916, \"labour syndicate\", \"labour syndicate\"], [16123, 16133, \"solidarity\", \"solidarity\"], [16135, 16148, \"direct action\", \"direct action\"], [16154, 16178, \"workers' self-management\", \"workers' self-management\"], [16181, 16204, \"Individualist anarchism\", \"Individualist anarchism\"], [16294, 16304, \"individual\", \"individual\"], [16315, 16319, \"will\", \"Will (philosophy)\"], [16422, 16436, \"William Godwin\", \"William Godwin\"], [16438, 16449, \"Max Stirner\", \"Max Stirner\"], [16454, 16473, \"Henry David Thoreau\", \"Henry David Thoreau\"], [16660, 16670, \"illegalism\", \"illegalism\"], [16675, 16697, \"individual reclamation\", \"individual reclamation\"], [16851, 16878, \"anti-globalization movement\", \"anti-globalization movement\"], [17105, 17115, \"black bloc\", \"black bloc\"], [17416, 17425, \"syncretic\", \"Syncretic politics\"], [17500, 17515, \"anti-capitalist\", \"anti-capitalist\"], [18210, 18227, \"anarcha-feminists\", \"anarcha-feminists\"], [18233, 18253, \"anarchist communists\", \"anarchist communists\"], [18862, 18879, \"the Establishment\", \"the Establishment\"], [19201, 19216, \"reject violence\", \"Nonviolence\"], [19332, 19337, \"Spain\", \"Spain\"], [19342, 19349, \"Ukraine\", \"Ukraine\"], [19382, 19391, \"terrorism\", \"terrorism\"], [19395, 19417, \"propaganda of the deed\", \"propaganda of the deed\"], [19543, 19553, \"revolution\", \"revolution\"], [19656, 19673, \"anarcho-pacifists\", \"Anarcho-pacifism\"], [19814, 19822, \"sabotage\", \"sabotage\"], [19921, 19927, \"tyrant\", \"tyrant\"], [19994, 20004, \"oppression\", \"oppression\"], [20028, 20040, \"Emma Goldman\", \"Emma Goldman\"], [20045, 20061, \"Errico Malatesta\", \"Errico Malatesta\"], [20176, 20190, \"necessary evil\", \"necessary evil\"], [20227, 20234, \"strikes\", \"Strike action\"], [20286, 20297, \"syndicalism\", \"syndicalism\"], [20312, 20321, \"reformist\", \"Reformism\"], [20391, 20396, \"state\", \"State (polity)\"], [20401, 20411, \"capitalism\", \"capitalism\"], [20497, 20503, \"nudism\", \"Naturism\"], [20553, 20563, \"friendship\", \"friendship\"], [20596, 20601, \"press\", \"News media\"], [20642, 20657, \"Alfredo Bonanno\", \"Alfredo Bonanno\"], [20674, 20699, \"insurrectionary anarchism\", \"insurrectionary anarchism\"], [20901, 20924, \"The Invisible Committee\", \"The Invisible Committee\"], [21463, 21469, \"Canada\", \"Anarchism in Canada\"], [21471, 21477, \"Mexico\", \"Anarchism in Mexico\"], [21481, 21487, \"Greece\", \"Anarchism in Greece\"], [21498, 21508, \"black bloc\", \"black bloc\"], [21691, 21710, \"anti-fascist action\", \"Anti-fascism\"], [21794, 21807, \"direct action\", \"direct action\"], [21876, 21885, \"hierarchy\", \"hierarchy\"], [22158, 22171, \"horizontalism\", \"Horizontalidad\"], [22233, 22243, \"grassroots\", \"grassroots\"], [22834, 22859, \"affinity anarchist groups\", \"Affinity group\"], [23633, 23648, \"wildcat strikes\", \"Wildcat strike action\"], [23741, 23747, \"online\", \"World Wide Web\"], [24275, 24280, \"squat\", \"Squatting\"], [24407, 24432, \"Temporary Autonomous Zone\", \"Temporary Autonomous Zone\"], [24454, 24464, \"surrealism\", \"surrealism\"], [24962, 24986, \"carnivalesque atmosphere\", \"Carnivalesque\"], [25055, 25065, \"philosophy\", \"philosophy\"], [25350, 25391, \"definitional concerns in anarchist theory\", \"definitional concerns in anarchist theory\"], [25428, 25438, \"capitalism\", \"Anarchism and capitalism\"], [25440, 25451, \"nationalism\", \"Anarchism and nationalism\"], [25456, 25464, \"religion\", \"Anarchism and religion\"], [25574, 25581, \"Marxism\", \"Anarchism and Marxism\"], [25583, 25592, \"communism\", \"Issues in anarchism#Communism\"], [25594, 25606, \"collectivism\", \"collectivism\"], [25611, 25625, \"trade unionism\", \"trade unionism\"], [25658, 25666, \"humanism\", \"humanism\"], [25668, 25684, \"divine authority\", \"God\"], [25686, 25711, \"enlightened self-interest\", \"enlightened self-interest\"], [25713, 25721, \"veganism\", \"Veganarchism\"], [25789, 25801, \"civilisation\", \"civilisation\"], [25803, 25813, \"technology\", \"technology\"], [25827, 25846, \"anarcho-primitivism\", \"anarcho-primitivism\"], [25856, 25874, \"democratic process\", \"Issues in anarchism#Participation in statist democracy\"], [26176, 26185, \"free love\", \"free love\"], [26261, 26270, \"polyamory\", \"polyamory\"], [26275, 26290, \"queer anarchism\", \"queer anarchism\"], [27106, 27114, \"suffrage\", \"suffrage\"], [27241, 27264, \"second wave of feminism\", \"Second-wave feminism\"], [27594, 27616, \"third wave of feminism\", \"Third-wave feminism\"], [27723, 27741, \"post-structuralist\", \"Post-structuralism\"], [28199, 28209, \"mutual aid\", \"Mutual aid (organization theory)\"], [28237, 28251, \"Willian Godwin\", \"William Godwin\"], [28256, 28267, \"Max Stirner\", \"Max Stirner\"], [28436, 28452, \"Francisco Ferrer\", \"Francisco Ferrer\"], [28469, 28484, \"Escuela Moderna\", \"Ferrer movement\"], [28861, 28880, \"class consciousness\", \"class consciousness\"], [29044, 29058, \"modern schools\", \"Modern School (United States)\"], [29097, 29108, \"Leo Tolstoy\", \"Leo Tolstoy\"], [29310, 29327, \"Summerhill School\", \"Summerhill School\"], [29984, 29994, \"Colin Ward\", \"Colin Ward\"], [29996, 30008, \"Herbert Read\", \"Herbert Read\"], [30013, 30025, \"Paul Goodman\", \"Paul Goodman\"], [30241, 30257, \"consumer society\", \"Consumerism\"], [30292, 30302, \"Colin Ward\", \"Colin Ward\"], [30687, 30692, \"state\", \"State (polity)\"], [30720, 30732, \"sine qua non\", \"sine qua non\"], [31067, 31071, \"open\", \"Open government\"], [31075, 31086, \"transparent\", \"Transparency (behavior)\"], [31466, 31478, \"ruling class\", \"ruling class\"], [31752, 31768, \"New Apocalyptics\", \"New Apocalyptics\"], [31777, 31792, \"Neo-romanticism\", \"Neo-romanticism\"], [32602, 32609, \"Anarchy\", \"Anarchy (magazine)\"], [33178, 33194, \"Bertrand Russell\", \"Bertrand Russell\"], [34724, 34739, \"A. John Simmons\", \"A. John Simmons\"], [35148, 35161, \"John Molyneux\", \"John Molyneux (academic)\"], [35162, 35186, \"Category:Anti-capitalism\", \"Category:Anti-capitalism\"], [35187, 35208, \"Category:Anti-fascism\", \"Category:Anti-fascism\"], [35209, 35237, \"Category:Economic ideologies\", \"Category:Economic ideologies\"], [35238, 35264, \"Category:Far-left politics\", \"Category:Far-left politics\"], [35265, 35295, \"Category:Libertarian socialism\", \"Category:Libertarian socialism\"], [35296, 35319, \"Category:Libertarianism\", \"Category:Libertarianism\"], [35320, 35346, \"Category:Political culture\", \"Category:Political culture\"], [35347, 35375, \"Category:Political movements\", \"Category:Political movements\"], [35376, 35405, \"Category:Political ideologies\", \"Category:Political ideologies\"], [35406, 35430, \"Category:Social theories\", \"Category:Social theories\"]]\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/entity_knowledge_in_bert\n",
    "import io\n",
    "import json\n",
    "import pickle\n",
    "import base64\n",
    "import bz2\n",
    "import json\n",
    "\n",
    "extracted_wiki_file = \"./downloads/enwiki-latest-pages-articles1.xml-p1p30303/AA/wiki_00\"\n",
    "with open(extracted_wiki_file) as f:\n",
    "    for wiki_article in f.readlines():\n",
    "        # each line is a dict with keys ['id', 'url', 'title', 'text', 'internal_links']\n",
    "        wiki_article = json.loads(wiki_article)\n",
    "#         print(wiki_article)\n",
    "#         print(wiki_article['id'] + '\\n')\n",
    "#         print(wiki_article['url'] + '\\n')\n",
    "#         print(wiki_article['title'] + '\\n')\n",
    "#         print(wiki_article['text'])\n",
    "\n",
    "        # The dictionary in 'internal_links' has begin and end char offsets as keys and \n",
    "        # then a tuple of mention (i.e. the link name) and entity (i.e. Wikipedia page \n",
    "        # name). Notice, f.ex. the line with \n",
    "        # \n",
    "        # (2317, 2328) ('Love Affair', 'Love Affair (1994 film)')\n",
    "        # \n",
    "        # where text 'Love Affair' was linking to the Wikipedia page 'Love Affair (1994 film)'\n",
    "\n",
    "        charoffsets_mentions = pickle.loads(base64.b64decode(wiki_article['internal_links'].encode('utf-8')))\n",
    "        mentions = [\n",
    "            (char_start, char_end, mention, wiki_page_name)\n",
    "            for ((char_start, char_end), (mention, wiki_page_name)) in charoffsets_mentions.items()\n",
    "        ]\n",
    "        print(json.dumps(mentions))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/entity_knowledge_in_bert/app\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/entity_knowledge_in_bert/app\n",
    "import base64\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import es\n",
    "\n",
    "# p = './downloads/enwiki-latest-pages-articles1.xml-p1p30303'\n",
    "\n",
    "# for dirname, dirnames, filenames in os.walk(p):\n",
    "#     # print path to all subdirectories first.\n",
    "# #     for subdirname in dirnames:\n",
    "# #         print(os.path.join(dirname, subdirname))\n",
    "\n",
    "#     # print path to all filenames.\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "        \n",
    "# fs = glob.glob('./downloads/enwiki-latest-pages-articles1.xml-p1p30303/**/*')\n",
    "# len(fs)\n",
    "\n",
    "es.connect()\n",
    "\n",
    "for p in glob.glob('../downloads/enwiki-latest-pages-articles1.xml-p1p30303/**/*'):\n",
    "    with open(p) as f:\n",
    "        for ln in f.readlines():\n",
    "            j = json.loads(ln)\n",
    "            charoffsets_mentions = pickle.loads(base64.b64decode(j['internal_links'].encode('utf-8')))\n",
    "            mentions = [\n",
    "                (char_start, char_end, mention, wiki_page_name)\n",
    "                for ((char_start, char_end), (mention, wiki_page_name)) in charoffsets_mentions.items()\n",
    "            ]\n",
    "            p = es.WikiPage(\n",
    "                title = j[\"title\"],\n",
    "                uid = j[\"id\"],\n",
    "                url = j[\"url\"],\n",
    "                text = j[\"text\"],\n",
    "                mentions=json.dumps(mentions, ensure_ascii=False))\n",
    "            p.save()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {1,2,3}\n",
    "b = {4,5}\n",
    "a |= b\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "65\n",
      "1\n",
      "3153\n",
      "2\n",
      "67399\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import elasticsearch\n",
    "import es\n",
    "es.connect()\n",
    "# es.WikiPage.search().query(\"match\", title=\"adobe\").execute()\n",
    "\n",
    "p = es.WikiPage.get(\"Adobe\")\n",
    "j = json.loads(p.mentions)\n",
    "\n",
    "visited = set()\n",
    "for m in j:\n",
    "    visited.add(m[3])\n",
    "# print(visited)\n",
    "\n",
    "def get_mentions(starts: set, excluded: set = set()) -> set:\n",
    "    founds = set()\n",
    "    for s in starts:\n",
    "        try:\n",
    "            p = es.WikiPage.get(s)\n",
    "#             print(f\"found: {s}\")\n",
    "        except elasticsearch.NotFoundError:\n",
    "            pass\n",
    "#             print(s)\n",
    "        else:\n",
    "            for m in json.loads(p.mentions):\n",
    "                wiki_title = m[3]\n",
    "                founds.add(wiki_title)\n",
    "#                 if wiki_title not in (starts | excluded):\n",
    "#                     founds.add(wiki_title)\n",
    "    return founds\n",
    "\n",
    "# es.WikiPage.get(\"\")\n",
    "\n",
    "starts = {\"Adobe\"}\n",
    "visited = set()\n",
    "for i in range(2):\n",
    "    print(i)\n",
    "    founds = get_mentions(starts, visited)\n",
    "    visited |= starts\n",
    "    starts = founds - visited\n",
    "    print(len(starts))\n",
    "# visited\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': 10000, 'relation': 'gte'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'a': defaultdict(int, {737073: 9, 737070: 1})})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl import Q\n",
    "import json\n",
    "import dateutil\n",
    "from collections import defaultdict\n",
    "\n",
    "client = Elasticsearch([\"es:9200\"])\n",
    "# s = Search(using=client, index=\"wiki-page\")\n",
    "s = Search(using=client, index=\"scraper-page\")\n",
    "q = Q('wildcard', resolved_url=\"*cnbc*\") & Q(\"term\", http_status=200)\n",
    "# q = Q(\"term\", title=\"adobe inc.\")\n",
    "# q = Q(\"match\", title=\"adobe inc\")\n",
    "s = s.filter(q)\n",
    "# s = s.sort('article_published_at')\n",
    "# s = s[101:200]\n",
    "resp = s.execute()\n",
    "# resp = s.params(preserve_order=True).scan()\n",
    "\n",
    "print(resp.hits.total)\n",
    "\n",
    "# for hit in s.filter(q).sort('-article_published_at').scan():\n",
    "# for i, hit in enumerate(s.scan()):\n",
    "# {\"aaa\": {\"2020-01-01\": 3, \"2020-01-04\": 1}, \"bbb\": {...}}\n",
    "\n",
    "count = defaultdict(lambda: defaultdict(int))\n",
    "for i, hit in enumerate(resp):\n",
    "    try:\n",
    "        d = dateutil.parser.parse(hit.article_published_at)\n",
    "        o = d.date().toordinal()\n",
    "        count['a'][o] += 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    if i > 100:\n",
    "        break\n",
    "# print(dates)\n",
    "count    \n",
    "\n",
    "# for hit in s.query(q).execute()[:10]:\n",
    "#     print(hit)\n",
    "# #     doc = doc.to_dict()\n",
    "#     print(hit.meta.to_dict())\n",
    "#     print(hit.title)\n",
    "#     print(doc[\"article_title\"], doc['article_text'])\n",
    "#     parsed = json.loads(doc[\"parsed\"])\n",
    "#     print(parsed[\"keywords\"])\n",
    "# dict(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 1, 14, 0, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dateutil import parser\n",
    "from collections import defaultdict\n",
    "a = defaultdict(lambda: defaultdict(int))\n",
    "a[\"a\"][\"a\"]\n",
    "parser.parse(\"2019-01-14T00:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "bb\n"
     ]
    }
   ],
   "source": [
    "x = \"aa\" + \"\\n\" + \"bb\"\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "t = datetime.datetime.now()\n",
    "# t.date().timestamp()\n",
    "# s = time.mktime(t.date().timetuple())\n",
    "# {d: [1,2,3]}\n",
    "# int(s)\n",
    "# datetime.datetime.fromtimestamp(int(s))\n",
    "\n",
    "# datetime.datetime.strptime(t)\n",
    "o = t.date().toordinal()\n",
    "type(o)\n",
    "# t.date().fromordinal(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Government spending', 'Congress', 'Apparel Retail', 'Business', 'Retail industry', 'Columbia Sportswear Co', 'Donald Trump', 'Politics', 'White House', 'business news']\n",
      "['Transportation', 'Asia News', 'Detroit Auto Show', 'Geely Automobile Holdings Ltd', 'Breaking News: Major', 'business news']\n",
      "['Personal loans', 'Tax planning', 'Government taxation and revenue', 'Investment strategy', 'Personal finance', 'business news']\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl import Q\n",
    "import json\n",
    "\n",
    "client = Elasticsearch(['es:9200'])\n",
    "s = Search(using=client, index=\"scraper-page-202008090431\")\n",
    "q = Q('wildcard', resolved_url=\"cnbc\") & Q(\"term\", http_status=200)\n",
    "\n",
    "for doc in s.execute()[0:3]:\n",
    "    doc = doc.to_dict()\n",
    "#     print(doc[\"article_title\"], doc['article_text'])\n",
    "    parsed = json.loads(doc[\"parsed\"])\n",
    "    print(parsed[\"keywords\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa': {'aa': 1, 'bb': 1, 'cc': 1},\n",
       " 'bb': {'aa': 1, 'bb': 1, 'cc': 1},\n",
       " 'cc': {'aa': 1, 'bb': 1, 'cc': 1}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count = {'aa': 1, 'bb': 2, 'cc': 3}\n",
    "bags = [['aa', 'bb'], ['cc']]\n",
    "bags = [e for bag in bags for e in bag]\n",
    "bags = [set(bags)]\n",
    "\n",
    "# cooccur = {'aa': {'aa': 3, 'bb': 2, 'cc': 3}}\n",
    "# for k, v in count.items():\n",
    "#     coocur[k][]\n",
    "# count.keys()\n",
    "\n",
    "cooccur = dict()\n",
    "for bag in bags:\n",
    "    for a in bag:\n",
    "        cooccur[a] = cooccur.get(a, dict())\n",
    "        for b in bag:\n",
    "            cooccur[a][b] = cooccur[a].get(b, 0) + 1\n",
    "cooccur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "a = {1,2,3,5}\n",
    "# 1 in a\n",
    "# a >= b\n",
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/entity_knowledge_in_bert/app/app\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/entity_knowledge_in_bert/app/app\n",
    "import re\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search, Q\n",
    "import spacy\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "client = Elasticsearch([\"es:9200\"])\n",
    "s = Search(using=client, index=\"scraper-page\")\n",
    "q = Q('wildcard', resolved_url=\"cnbc\") & Q(\"term\", http_status=200)\n",
    "# q = Q(\"term\", title=\"adobe inc.\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# text = 'My first birthday was great. My 2. was even better.'\n",
    "# [sent.text for sent in nlp(text).sents]\n",
    "\n",
    "\n",
    "kws = {\"software\": [\"computer software\", \"software\", \"installed programs\"], \"San Jose, California\": [\"San Jose, California\", \"San Jose\"], \"Delaware\": [\"Delaware\"], \"Adobe Flash\": [\"Adobe Flash\", \"Flash\"], \"Photoshop\": [\"Photoshop Mix\", \"Photoshop\"]}\n",
    "processor = KeywordProcessor()\n",
    "processor.add_keywords_from_dict(kws)\n",
    "\n",
    "def get_cooccur(bag: str, cooccur = dict()):\n",
    "    for a in bag:\n",
    "        cooccur[a] = cooccur.get(a, dict())\n",
    "        for b in bag:\n",
    "            cooccur[a][b] = cooccur[a].get(b, 0) + 1\n",
    "    return cooccur\n",
    "\n",
    "def count_sent(sent):\n",
    "    kws = processor.extract_keywords(sent)\n",
    "    atk_bags.append(kws)\n",
    "    return get_cooccur(kws)\n",
    "\n",
    "\n",
    "sent_cooccur = {}\n",
    "atk_coocur = {}\n",
    "atk_bags = []\n",
    "# freq_count = \n",
    "\n",
    "done_urls = set()\n",
    "for hit in s.query(q).execute()[:10]:\n",
    "    if hit.resolved_url in done_urls:\n",
    "        continue\n",
    "\n",
    "    sents = []\n",
    "    try:\n",
    "        sents.append(hit.article_title)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        sents += [sent.text for sent in nlp(hit.article_text).sents]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    atk_bag = set()\n",
    "    for sent in sents:\n",
    "        bag = processor.extract_keywords(sent, span_info=False)\n",
    "        atk_bag |= set(bag)\n",
    "        sent_coocur = get_cooccur(bag, sent_coocur)\n",
    "    atk_coocur = get_cooccur(atk_bag, atk_coocur)\n",
    "    atk_bags.append(atk_bag)\n",
    "    #     print(hit.article_published_at)\n",
    "\n",
    "    done_urls.add(hit.resolved_url)\n",
    "#     print(hit.resolved_url)\n",
    "#     print(hit.article_parsed)\n",
    "#         print(hit.article_text.split('/n'))\n",
    "#     print(re.split('/n|\\n', hit.article_text))\n",
    "#     sents = [sent.text for sent in nlp(hit.article_text).sents]\n",
    "#     sents = [sent.text for sent in nlp(hit.article_text).sents]\n",
    "#     print(hit.article_text)\n",
    "#     print(sents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ticker to comapny names\n",
    "\n",
    "Get all tickers\n",
    "* http://markets.cboe.com/us/equities/market_statistics/symbols_traded/?mkt=byx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "msft = yf.Ticker(\"RXT\")\n",
    "# name = msft.info['shortName']  # 'Microsoft Corporation'\n",
    "# msft.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/entity_knowledge_in_bert/app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pk_286936170c0d42818fd3dc57f76ce0a0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /workspace/entity_knowledge_in_bert/app\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.getenv(\"IEXCLOUD_TOKEN\")\n",
    "# DATABASE_PASSWORD = os.getenv(\"DATABASE_PASSWORD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'symbol': 'RXT',\n",
       " 'companyName': 'Rackspace Technology, Inc.',\n",
       " 'exchange': 'NASDAQ',\n",
       " 'industry': 'Data Processing Services',\n",
       " 'website': 'http://www.rackspace.com',\n",
       " 'description': 'Rackspace Technology, Inc. engages in the provision of end-to-end multi-cloud technology services. The firm designs, builds and operates its customers’ cloud environments across technology platforms. It operates through the following segments: Multicloud Services, Apps and Cross Platform, and OpenStack Public Cloud. The company was founded on July 21, 2016 and is headquartered in San Antonio, TX.',\n",
       " 'CEO': 'Kevin M. Jones',\n",
       " 'securityName': 'Rackspace Technology, Inc.',\n",
       " 'issueType': 'cs',\n",
       " 'sector': 'Technology Services',\n",
       " 'primarySicCode': 7374,\n",
       " 'employees': None,\n",
       " 'tags': ['Technology Services', 'Data Processing Services'],\n",
       " 'address': '1 Fanatical Place',\n",
       " 'address2': None,\n",
       " 'state': 'TX',\n",
       " 'city': 'San Antonio',\n",
       " 'zip': '78218',\n",
       " 'country': 'US',\n",
       " 'phone': '1.210.312.4000'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import requests_cache\n",
    "requests_cache.install_cache('cache')\n",
    "\n",
    "stock = \"rxt\"\n",
    "\n",
    "r = requests.get(f\"https://cloud.iexapis.com/stable/stock/{stock}/company\", params=params)\n",
    "j = r.json()\n",
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rackspace Rackspace PROPN NNP compound Xxxxx True False\n",
      "Technology Technology PROPN NNP ROOT Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "Inc. Inc. PROPN NNP pobj Xxx. False False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Rackspace', 'Technology', ',', 'Inc.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = \"Rackspace Technology, Inc.\"\n",
    "\n",
    "\n",
    "# for c in \",.\":\n",
    "#     s = s.replace(c, \"\")\n",
    "# s = s.strip()\n",
    "# s.lower()\n",
    "\n",
    "# all_stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "# text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "doc = nlp(s)\n",
    "# tokens_without_sw= [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "# print(tokens_without_sw)\n",
    "# print(doc)\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)\n",
    "# [tk.text for tk in doc if (not tk.is_stop or not tk.pos_ == \"PUNCT\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rackspace Technology, Inc.',\n",
       " 'Rackspace Technology Inc',\n",
       " 'rackspace technology']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def candidates(term):\n",
    "    cands = [term]\n",
    "\n",
    "    # remove punct\n",
    "    for c in \",.:;\":\n",
    "        term = term.replace(c, \"\")\n",
    "    term = term.strip()\n",
    "    cands.append(term)\n",
    "\n",
    "    # remove extra words\n",
    "    term = term.lower()\n",
    "    for w in (\"inc\", \"limited\", \"ltd\"):\n",
    "        term = term.replace(w, \"\")\n",
    "    term = term.strip()\n",
    "    cands.append(term)\n",
    "\n",
    "    return cands\n",
    "\n",
    "name = 'Rackspace Technology, Inc.'\n",
    "candidates(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Microsoft'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import requests_cache\n",
    "requests_cache.install_cache('cache')\n",
    "\n",
    "name = 'Rackspace Technology, Inc.'\n",
    "api = \"https://en.wikipedia.org/w/api.php\"\n",
    "params={\n",
    "    \"action\": \"opensearch\",\n",
    "    \"namespace\": \"0\",\n",
    "    \"search\": \"Microsoft Corporation\",\n",
    "    \"limit\": 1,\n",
    "    \"format\": \"json\",\n",
    "    \"redirects\": \"resolve\",\n",
    "\n",
    "#     'action': 'opensearch',\n",
    "#     'search':'Microsoft Corporation',\n",
    "#     'limit': 1,\n",
    "#     'namespace': 0, \n",
    "#     'format': 'json',\n",
    "}\n",
    "\n",
    "# print(api)\n",
    "r = requests.get(api, params=params)\n",
    "j = r.json()\n",
    "url = j[3][0]  # 'https://en.wikipedia.org/wiki/Microsoft'\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/workspace/entity_knowledge_in_bert/app/downloads/bats_symbols_traded_byx.csv\")\n",
    "# list(df[\"Symbols\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': {'b'}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {\"a\": {\"b\"}}\n",
    "a[\"a\"] = a.get(\"a\", set()) | {\"b\"}\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
