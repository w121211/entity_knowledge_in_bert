{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install sparqlwrapper\n",
    "# !pip install wikidataintegrator\n",
    "# !pip install --upgrade --force-reinstall git+https://github.com/vi3k6i5/flashtext.git\n",
    "# !pip install rdflib\n",
    "# !pip install requests_cache\n",
    "# !conda install -y -c ranaroussi yfinance\n",
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wiki-data\n",
    "\n",
    "wiki-data api  \n",
    "https://query.wikidata.org/  \n",
    "https://sqid.toolforge.org/#/view?id=P452  \n",
    "\n",
    "https://www.wikidata.org/w/api.php?action=wbgetclaims&entity=Q66&format=jsonfm   \n",
    "https://www.wikidata.org/w/api.php?action=wbgetentities&ids=Q66&format=jsonfm \n",
    "\n",
    "```sparql\n",
    "# 列items有ticker property\n",
    "SELECT ?item ?itemLabel ?value ?valueLabel\n",
    "WHERE \n",
    "{\n",
    "  # wdt:P249 (ticker symbol)\n",
    "  ?item wdt:P249 ?value\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "LIMIT 10\n",
    "\n",
    "#\n",
    "?horse wdt:P31/wdt:P279* wd:Q726 .\n",
    " wd:Q180816\t\n",
    "```\n",
    "\n",
    "# Wiki\n",
    "\n",
    "wiki-page api  \n",
    "https://en.wikipedia.org/wiki/Special:ApiSandbox#action=query&prop=revisions&titles=Pet_door&rvslots=*&rvprop=content&formatversion=2\n",
    "\n",
    "Query wiki-page  \n",
    "https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro&explaintext&redirects=1&pageids=21721040\n",
    "\n",
    "用wikidata-id找對應的wikipage  \n",
    "https://www.wikidata.org/w/api.php?action=wbgetentities&format=xml&props=sitelinks&ids=Q19675&sitefilter=frwiki\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/entity_knowledge_in_bert\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Self-driving_car'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 從具有company屬性的節點開始，抓5層mentions\n",
    "\n",
    "# 取得上市公司entities\n",
    "#  方法1：掃過整個wikipedia-dump，找出具有company屬性的entities\n",
    "#  方法2（採用）：query wikidata找company屬性的entities\n",
    "\n",
    "# 取得跟公司關聯的mentions（n階層）\n",
    "# 掃wikipedia-dump，從company的wiki-page開始抓裡面的mentions\n",
    "#  將新加入的mentions設為next_entities，重複抓取n次（=爬n層）\n",
    "\n",
    "%cd /workspace/entity_knowledge_in_bert\n",
    "import requests\n",
    "import requests_cache\n",
    "requests_cache.install_cache('cache')\n",
    "\n",
    "j = requests.get(\"https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&props=sitelinks&ids=Q741490\").json()\n",
    "a = j['entities']['Q741490']['sitelinks']['enwiki']['title'].replace(' ', '_')\n",
    "a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認爬出來的entities\n",
    "#  先確認一下entities的內容，是否足夠滿足#tag需求（ie足夠豐富、多元）\n",
    "#  假如不夠，可考慮採關鍵字方式與wiki-entity-dictionary做對照\n",
    "\n",
    "# 用flashtext從news-corpus中抓bags\n",
    "#  sentence-bag: 同時出現在1句子中的entities\n",
    "#  paragraph-bag: 同時出現在1段落中的entities\n",
    "#  sentence-coocuur-dict\n",
    "#  paragraph-coocuur-dict\n",
    "#  freq-daily-count-dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"#aaa\"\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbpedia infobox\n",
    "!cd /workspace/entity_knowledge_in_bert\n",
    "import bz2\n",
    "# import rdflib\n",
    "# from rdflib import Graph\n",
    "\n",
    "# f = \"./downloads/infobox-property-definitions_lang=en.ttl.bz2\"\n",
    "# f = \"./downloads/mappingbased-objects_lang=en.ttl.bz2\"\n",
    "# f = \"./downloads/wikipedia-links_lang=en.ttl.bz2\"\n",
    "f = \"./downloads/redirects_lang=en.ttl.bz2\"\n",
    "# f = \"./downloads/enwiki-latest-pages-articles1.xml-p1p30303.bz2\"\n",
    "# f = \"./downloads/disambiguations_en.ttl.bz2\"\n",
    "\n",
    "# g = Graph()\n",
    "# g.parse(f, format=\"turtle\"); #what is right value for the format\n",
    "\n",
    "with bz2.BZ2File(f, \"rb\") as f:\n",
    "    for i, l in enumerate(f):\n",
    "#         if i > 10:\n",
    "#             break\n",
    "        line = l.decode().strip()\n",
    "#         print(line)\n",
    "        if line[0] == '#':\n",
    "            continue\n",
    "        try:\n",
    "            s, p, o, _ = line.split(\" \")\n",
    "#             print(s, p, o)\n",
    "        except ValueError:\n",
    "            print(line)\n",
    "        \n",
    "#         if \"http://dbpedia.org/resource/Google\" in s or \"http://dbpedia.org/resource/Google\" in o:\n",
    "        if \"<http://dbpedia.org/resource/boeing>\" in s:\n",
    "            print(s, p, o)\n",
    "\n",
    "#         if \"http://dbpedia.org/resource/Google\" in line_decoded:\n",
    "#             print(line_decoded)\n",
    "        \n",
    "# #             for line in tqdm.tqdm(file):\n",
    "# #                 line_decoded = line.decode().strip()\n",
    "# #                 matcher_match = matcher.match(line_decoded)\n",
    "# #                 if matcher_match:\n",
    "# #                     if matcher_match.group(match_val) in redirects:\n",
    "# #                         a_to_b[\n",
    "# #                             postproc_key(matcher_match.group(match_key))\n",
    "# #                         ].append(redirects[postproc_val(matcher_match.group(match_val))])\n",
    "# #                     else:\n",
    "# #                         a_to_b[\n",
    "# #                             postproc_key(matcher_match.group(match_key))\n",
    "# #                         ].append(postproc_val(matcher_match.group(match_val)))\n",
    "# #         return a_to_b\n",
    "# for s, p, o in g:\n",
    "#     print(s, p, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flashtext\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "text = \"Car When Sebastian Thrun started working on self driving car at Google in 2007, few people outside of the company took him seriously\"\n",
    "\n",
    "keyword_processor = KeywordProcessor()\n",
    "# keyword_processor.add_keyword('Big Apple', 'New York')\n",
    "# keyword_processor.add_keyword('apple', 'apple watch')\n",
    "# keyword_processor.add_keyword(\"Google\")\n",
    "# keyword_processor.add_keyword(\"Sebastian Thrun\")\n",
    "keyword_processor.add_keyword(\"Self driving (car)\")\n",
    "# keyword_processor.add_keyword(\"cars\", \"car\")\n",
    "# keyword_processor.add_keyword(\"car\", \"car\")\n",
    "\n",
    "# keyword_processor.add_keyword(\"apple \", \"apple-watch\")\n",
    "\n",
    "# keywords_found = keyword_processor.extract_keywords(text, span_info=True, max_cost=1)\n",
    "# keywords_found = keyword_processor.extract_keywords(text, span_info=False, max_cost=1)\n",
    "keywords_found = keyword_processor.extract_keywords(text, span_info=False)\n",
    "keywords_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "def get_results(endpoint_url, query):\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    # TODO adjust user agent; see https://w.wiki/CX6\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qeury具有stock exchange（wdt:P414）屬性的items\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT ?item ?itemLabel ?value ?valueLabel\n",
    "WHERE \n",
    "{\n",
    "  ?item wdt:P414 ?value\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "results = get_results(endpoint_url, query)\n",
    "# results[\"results\"][\"bindings\"]\n",
    "\n",
    "# 將results儲存在本機\n",
    "import json\n",
    "with open('data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/flair\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q66'},\n",
       " 'value': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q13677'},\n",
       " 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Boeing'},\n",
       " 'valueLabel': {'xml:lang': 'en',\n",
       "  'type': 'literal',\n",
       "  'value': 'New York Stock Exchange'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /workspace/flair\n",
    "import json\n",
    "\n",
    "with open('data.json',  encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# len(results[\"results\"][\"bindings\"])\n",
    "data[\"results\"][\"bindings\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得entity statements（claims）\n",
    "import requests\n",
    "import \n",
    "\n",
    "entity = 'Q66'\n",
    "endpoint = f'https://www.wikidata.org/w/api.php?action=wbgetclaims&entity={entity}&format=json'\n",
    "\n",
    "r = requests.get(endpoint)\n",
    "d = r.json()\n",
    "# d['claims']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得entity statements（claims） - 用wikidataintegrator\n",
    "from wikidataintegrator import wdi_core\n",
    "\n",
    "# wdid='Q707530'\n",
    "wdid=\"Q741490\"\n",
    "my_first_wikidata_item = wdi_core.WDItemEngine(wd_item_id=wdid, search_only=True)\n",
    "\n",
    "# to check successful installation and retrieval of the data, you can print the json representation of the item\n",
    "d = my_first_wikidata_item.get_wd_json_representation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d['claims']['P169'][-1]\n",
    "# d['claims']\n",
    "# ['mainsnak']['datavalue']['value']['id']\n",
    "# my_first_wikidata_item.mrh\n",
    "# d['claims']\n",
    "# del d['claims']\n",
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /workspace/entity_knowledge_in_bert/wikiextractor-wikimentions\n",
    "# !python WikiExtractor.py --json --filter_disambig_pages --processes 2 --collect_links ../downloads/enwiki-latest-pages-articles1.xml-p1p30303.bz2 -o test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspace/entity_knowledge_in_bert\n",
    "import io\n",
    "import json\n",
    "import pickle\n",
    "import base64\n",
    "import bz2\n",
    "import json\n",
    "\n",
    "extracted_wiki_file = \"./downloads/enwiki-latest-pages-articles1.xml-p1p30303/AA/wiki_00\"\n",
    "with open(extracted_wiki_file) as f:\n",
    "    for wiki_article in f.readlines():\n",
    "        # each line is a dict with keys ['id', 'url', 'title', 'text', 'internal_links']\n",
    "        wiki_article = json.loads(wiki_article)\n",
    "#         print(wiki_article)\n",
    "#         print(wiki_article['id'] + '\\n')\n",
    "#         print(wiki_article['url'] + '\\n')\n",
    "#         print(wiki_article['title'] + '\\n')\n",
    "#         print(wiki_article['text'])\n",
    "\n",
    "        # The dictionary in 'internal_links' has begin and end char offsets as keys and \n",
    "        # then a tuple of mention (i.e. the link name) and entity (i.e. Wikipedia page \n",
    "        # name). Notice, f.ex. the line with \n",
    "        # \n",
    "        # (2317, 2328) ('Love Affair', 'Love Affair (1994 film)')\n",
    "        # \n",
    "        # where text 'Love Affair' was linking to the Wikipedia page 'Love Affair (1994 film)'\n",
    "\n",
    "        charoffsets_mentions = pickle.loads(base64.b64decode(wiki_article['internal_links'].encode('utf-8')))\n",
    "        mentions = [\n",
    "            (char_start, char_end, mention, wiki_page_name)\n",
    "            for ((char_start, char_end), (mention, wiki_page_name)) in charoffsets_mentions.items()\n",
    "        ]\n",
    "        print(json.dumps(mentions))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/entity_knowledge_in_bert/app\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/entity_knowledge_in_bert/app\n",
    "import base64\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import es\n",
    "\n",
    "# p = './downloads/enwiki-latest-pages-articles1.xml-p1p30303'\n",
    "\n",
    "# for dirname, dirnames, filenames in os.walk(p):\n",
    "#     # print path to all subdirectories first.\n",
    "# #     for subdirname in dirnames:\n",
    "# #         print(os.path.join(dirname, subdirname))\n",
    "\n",
    "#     # print path to all filenames.\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "        \n",
    "# fs = glob.glob('./downloads/enwiki-latest-pages-articles1.xml-p1p30303/**/*')\n",
    "# len(fs)\n",
    "\n",
    "es.connect()\n",
    "\n",
    "for p in glob.glob('../downloads/enwiki-latest-pages-articles1.xml-p1p30303/**/*'):\n",
    "    with open(p) as f:\n",
    "        for ln in f.readlines():\n",
    "            j = json.loads(ln)\n",
    "            charoffsets_mentions = pickle.loads(base64.b64decode(j['internal_links'].encode('utf-8')))\n",
    "            mentions = [\n",
    "                (char_start, char_end, mention, wiki_page_name)\n",
    "                for ((char_start, char_end), (mention, wiki_page_name)) in charoffsets_mentions.items()\n",
    "            ]\n",
    "            p = es.WikiPage(\n",
    "                title = j[\"title\"],\n",
    "                uid = j[\"id\"],\n",
    "                url = j[\"url\"],\n",
    "                text = j[\"text\"],\n",
    "                mentions=json.dumps(mentions, ensure_ascii=False))\n",
    "            p.save()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': 10000, 'relation': 'gte'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'a': defaultdict(int, {737073: 9, 737070: 1})})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl import Q\n",
    "import json\n",
    "import dateutil\n",
    "from collections import defaultdict\n",
    "\n",
    "client = Elasticsearch([\"es:9200\"])\n",
    "# s = Search(using=client, index=\"wiki-page\")\n",
    "s = Search(using=client, index=\"scraper-page\")\n",
    "q = Q('wildcard', resolved_url=\"*cnbc*\") & Q(\"term\", http_status=200)\n",
    "# q = Q(\"term\", title=\"adobe inc.\")\n",
    "# q = Q(\"match\", title=\"adobe inc\")\n",
    "s = s.filter(q)\n",
    "# s = s.sort('article_published_at')\n",
    "# s = s[101:200]\n",
    "resp = s.execute()\n",
    "# resp = s.params(preserve_order=True).scan()\n",
    "\n",
    "print(resp.hits.total)\n",
    "\n",
    "# for hit in s.filter(q).sort('-article_published_at').scan():\n",
    "# for i, hit in enumerate(s.scan()):\n",
    "# {\"aaa\": {\"2020-01-01\": 3, \"2020-01-04\": 1}, \"bbb\": {...}}\n",
    "\n",
    "count = defaultdict(lambda: defaultdict(int))\n",
    "for i, hit in enumerate(resp):\n",
    "    try:\n",
    "        d = dateutil.parser.parse(hit.article_published_at)\n",
    "        o = d.date().toordinal()\n",
    "        count['a'][o] += 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    if i > 100:\n",
    "        break\n",
    "# print(dates)\n",
    "count    \n",
    "\n",
    "# for hit in s.query(q).execute()[:10]:\n",
    "#     print(hit)\n",
    "# #     doc = doc.to_dict()\n",
    "#     print(hit.meta.to_dict())\n",
    "#     print(hit.title)\n",
    "#     print(doc[\"article_title\"], doc['article_text'])\n",
    "#     parsed = json.loads(doc[\"parsed\"])\n",
    "#     print(parsed[\"keywords\"])\n",
    "# dict(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 1, 14, 0, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dateutil import parser\n",
    "from collections import defaultdict\n",
    "a = defaultdict(lambda: defaultdict(int))\n",
    "a[\"a\"][\"a\"]\n",
    "parser.parse(\"2019-01-14T00:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "bb\n"
     ]
    }
   ],
   "source": [
    "x = \"aa\" + \"\\n\" + \"bb\"\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "t = datetime.datetime.now()\n",
    "# t.date().timestamp()\n",
    "# s = time.mktime(t.date().timetuple())\n",
    "# {d: [1,2,3]}\n",
    "# int(s)\n",
    "# datetime.datetime.fromtimestamp(int(s))\n",
    "\n",
    "# datetime.datetime.strptime(t)\n",
    "o = t.date().toordinal()\n",
    "type(o)\n",
    "# t.date().fromordinal(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Government spending', 'Congress', 'Apparel Retail', 'Business', 'Retail industry', 'Columbia Sportswear Co', 'Donald Trump', 'Politics', 'White House', 'business news']\n",
      "['Transportation', 'Asia News', 'Detroit Auto Show', 'Geely Automobile Holdings Ltd', 'Breaking News: Major', 'business news']\n",
      "['Personal loans', 'Tax planning', 'Government taxation and revenue', 'Investment strategy', 'Personal finance', 'business news']\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl import Q\n",
    "import json\n",
    "\n",
    "client = Elasticsearch(['es:9200'])\n",
    "s = Search(using=client, index=\"scraper-page-202008090431\")\n",
    "q = Q('wildcard', resolved_url=\"cnbc\") & Q(\"term\", http_status=200)\n",
    "\n",
    "for doc in s.execute()[0:3]:\n",
    "    doc = doc.to_dict()\n",
    "#     print(doc[\"article_title\"], doc['article_text'])\n",
    "    parsed = json.loads(doc[\"parsed\"])\n",
    "    print(parsed[\"keywords\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa': {'aa': 1, 'bb': 1, 'cc': 1},\n",
       " 'bb': {'aa': 1, 'bb': 1, 'cc': 1},\n",
       " 'cc': {'aa': 1, 'bb': 1, 'cc': 1}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count = {'aa': 1, 'bb': 2, 'cc': 3}\n",
    "bags = [['aa', 'bb'], ['cc']]\n",
    "bags = [e for bag in bags for e in bag]\n",
    "bags = [set(bags)]\n",
    "\n",
    "# cooccur = {'aa': {'aa': 3, 'bb': 2, 'cc': 3}}\n",
    "# for k, v in count.items():\n",
    "#     coocur[k][]\n",
    "# count.keys()\n",
    "\n",
    "cooccur = dict()\n",
    "for bag in bags:\n",
    "    for a in bag:\n",
    "        cooccur[a] = cooccur.get(a, dict())\n",
    "        for b in bag:\n",
    "            cooccur[a][b] = cooccur[a].get(b, 0) + 1\n",
    "cooccur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "a = {1,2,3,5}\n",
    "# 1 in a\n",
    "# a >= b\n",
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/entity_knowledge_in_bert/app/app\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n",
      "[]\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/entity_knowledge_in_bert/app/app\n",
    "import re\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search, Q\n",
    "import spacy\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "client = Elasticsearch([\"es:9200\"])\n",
    "s = Search(using=client, index=\"scraper-page\")\n",
    "q = Q('wildcard', resolved_url=\"cnbc\") & Q(\"term\", http_status=200)\n",
    "# q = Q(\"term\", title=\"adobe inc.\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# text = 'My first birthday was great. My 2. was even better.'\n",
    "# [sent.text for sent in nlp(text).sents]\n",
    "\n",
    "\n",
    "kws = {\"software\": [\"computer software\", \"software\", \"installed programs\"], \"San Jose, California\": [\"San Jose, California\", \"San Jose\"], \"Delaware\": [\"Delaware\"], \"Adobe Flash\": [\"Adobe Flash\", \"Flash\"], \"Photoshop\": [\"Photoshop Mix\", \"Photoshop\"]}\n",
    "processor = KeywordProcessor()\n",
    "processor.add_keywords_from_dict(kws)\n",
    "\n",
    "def get_cooccur(bag: str, cooccur = dict()):\n",
    "    for a in bag:\n",
    "        cooccur[a] = cooccur.get(a, dict())\n",
    "        for b in bag:\n",
    "            cooccur[a][b] = cooccur[a].get(b, 0) + 1\n",
    "    return cooccur\n",
    "\n",
    "def count_sent(sent):\n",
    "    kws = processor.extract_keywords(sent)\n",
    "    atk_bags.append(kws)\n",
    "    return get_cooccur(kws)\n",
    "\n",
    "\n",
    "sent_cooccur = {}\n",
    "atk_coocur = {}\n",
    "atk_bags = []\n",
    "# freq_count = \n",
    "\n",
    "done_urls = set()\n",
    "for hit in s.query(q).execute()[:10]:\n",
    "    if hit.resolved_url in done_urls:\n",
    "        continue\n",
    "\n",
    "    sents = []\n",
    "    try:\n",
    "        sents.append(hit.article_title)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        sents += [sent.text for sent in nlp(hit.article_text).sents]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    atk_bag = set()\n",
    "    for sent in sents:\n",
    "        bag = processor.extract_keywords(sent, span_info=False)\n",
    "        atk_bag |= set(bag)\n",
    "        sent_coocur = get_cooccur(bag, sent_coocur)\n",
    "    atk_coocur = get_cooccur(atk_bag, atk_coocur)\n",
    "    atk_bags.append(atk_bag)\n",
    "    #     print(hit.article_published_at)\n",
    "\n",
    "    done_urls.add(hit.resolved_url)\n",
    "#     print(hit.resolved_url)\n",
    "#     print(hit.article_parsed)\n",
    "#         print(hit.article_text.split('/n'))\n",
    "#     print(re.split('/n|\\n', hit.article_text))\n",
    "#     sents = [sent.text for sent in nlp(hit.article_text).sents]\n",
    "#     sents = [sent.text for sent in nlp(hit.article_text).sents]\n",
    "#     print(hit.article_text)\n",
    "#     print(sents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ticker to comapny names\n",
    "\n",
    "Get all tickers\n",
    "* http://markets.cboe.com/us/equities/market_statistics/symbols_traded/?mkt=byx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "msft = yf.Ticker(\"RXT\")\n",
    "# name = msft.info['shortName']  # 'Microsoft Corporation'\n",
    "# msft.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/entity_knowledge_in_bert/app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pk_286936170c0d42818fd3dc57f76ce0a0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /workspace/entity_knowledge_in_bert/app\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.getenv(\"IEXCLOUD_TOKEN\")\n",
    "# DATABASE_PASSWORD = os.getenv(\"DATABASE_PASSWORD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'symbol': 'RXT',\n",
       " 'companyName': 'Rackspace Technology, Inc.',\n",
       " 'exchange': 'NASDAQ',\n",
       " 'industry': 'Data Processing Services',\n",
       " 'website': 'http://www.rackspace.com',\n",
       " 'description': 'Rackspace Technology, Inc. engages in the provision of end-to-end multi-cloud technology services. The firm designs, builds and operates its customers’ cloud environments across technology platforms. It operates through the following segments: Multicloud Services, Apps and Cross Platform, and OpenStack Public Cloud. The company was founded on July 21, 2016 and is headquartered in San Antonio, TX.',\n",
       " 'CEO': 'Kevin M. Jones',\n",
       " 'securityName': 'Rackspace Technology, Inc.',\n",
       " 'issueType': 'cs',\n",
       " 'sector': 'Technology Services',\n",
       " 'primarySicCode': 7374,\n",
       " 'employees': None,\n",
       " 'tags': ['Technology Services', 'Data Processing Services'],\n",
       " 'address': '1 Fanatical Place',\n",
       " 'address2': None,\n",
       " 'state': 'TX',\n",
       " 'city': 'San Antonio',\n",
       " 'zip': '78218',\n",
       " 'country': 'US',\n",
       " 'phone': '1.210.312.4000'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import requests_cache\n",
    "requests_cache.install_cache('cache')\n",
    "\n",
    "stock = \"rxt\"\n",
    "\n",
    "r = requests.get(f\"https://cloud.iexapis.com/stable/stock/{stock}/company\", params=params)\n",
    "j = r.json()\n",
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rackspace Rackspace PROPN NNP compound Xxxxx True False\n",
      "Technology Technology PROPN NNP ROOT Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "Inc. Inc. PROPN NNP pobj Xxx. False False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Rackspace', 'Technology', ',', 'Inc.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "s = \"Rackspace Technology, Inc.\"\n",
    "\n",
    "\n",
    "# for c in \",.\":\n",
    "#     s = s.replace(c, \"\")\n",
    "# s = s.strip()\n",
    "# s.lower()\n",
    "\n",
    "# all_stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "# text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "doc = nlp(s)\n",
    "# tokens_without_sw= [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "# print(tokens_without_sw)\n",
    "# print(doc)\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)\n",
    "# [tk.text for tk in doc if (not tk.is_stop or not tk.pos_ == \"PUNCT\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rackspace Technology, Inc.',\n",
       " 'Rackspace Technology Inc',\n",
       " 'rackspace technology']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def candidates(term):\n",
    "    cands = [term]\n",
    "\n",
    "    # remove punct\n",
    "    for c in \",.:;\":\n",
    "        term = term.replace(c, \"\")\n",
    "    term = term.strip()\n",
    "    cands.append(term)\n",
    "\n",
    "    # remove extra words\n",
    "    term = term.lower()\n",
    "    for w in (\"inc\", \"limited\", \"ltd\"):\n",
    "        term = term.replace(w, \"\")\n",
    "    term = term.strip()\n",
    "    cands.append(term)\n",
    "\n",
    "    return cands\n",
    "\n",
    "name = 'Rackspace Technology, Inc.'\n",
    "candidates(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Microsoft'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import requests_cache\n",
    "requests_cache.install_cache('cache')\n",
    "\n",
    "name = 'Rackspace Technology, Inc.'\n",
    "api = \"https://en.wikipedia.org/w/api.php\"\n",
    "params={\n",
    "    \"action\": \"opensearch\",\n",
    "    \"namespace\": \"0\",\n",
    "    \"search\": \"Microsoft Corporation\",\n",
    "    \"limit\": 1,\n",
    "    \"format\": \"json\",\n",
    "    \"redirects\": \"resolve\",\n",
    "#     'action': 'opensearch',\n",
    "#     'search':'Microsoft Corporation',\n",
    "#     'limit': 1,\n",
    "#     'namespace': 0, \n",
    "#     'format': 'json',\n",
    "}\n",
    "\n",
    "# print(api)\n",
    "r = requests.get(api, params=params)\n",
    "j = r.json()\n",
    "url = j[3][0]  # 'https://en.wikipedia.org/wiki/Microsoft'\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filepath = \"/workspace/entity_knowledge_in_bert/app/outputs/wiki_mentions.json\"\n",
    "with open(filepath, encoding='utf-8') as f:\n",
    "    j = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "653773"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# df = pd.DataFrame({\"name\": j['seeds'][1]})\n",
    "# df.to_csv('/workspace/entity_knowledge_in_bert/app/outputs/seeds_1.csv', index=False)\n",
    "len(j['seeds'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/entity_knowledge_in_bert/app\n",
      "Depth 0: 1 nodes\n",
      "Depth 1: 65 nodes\n",
      "Depth 2: 3153 nodes\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/entity_knowledge_in_bert/app\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import elasticsearch\n",
    "import networkx as nx\n",
    "\n",
    "from app import es\n",
    "from app import preprocess\n",
    "\n",
    "es.connect([\"es:9200\"])\n",
    "G = preprocess.get_wikipage_mentions([\"Adobe\"], depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.DiGraph()\n",
    "G.add_node(\"aaa\", test=1)\n",
    "for n in G.nodes:\n",
    "    G.nodes[n][\"test\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
